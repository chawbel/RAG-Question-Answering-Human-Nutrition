{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the document"
      ],
      "metadata": {
        "id": "tyFOIA5_WX5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index pymupdf langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install rank-bm25\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "!pip install langchain-community\n",
        "!pip install bitsandbytes accelerate\n",
        "!pip install uvicorn\n",
        "!pip install fastapi\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "r_bw2zPTzLSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "document = SimpleDirectoryReader(\n",
        "    input_files = [\"human-nutrition-text.pdf\"]\n",
        ").load_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "oohzfS1OdUaB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "import fitz\n",
        "\n",
        "def extract_text_with_ocr(pdf_path):\n",
        "  doc = fitz.open(pdf_path)\n",
        "  full_text = \"\"\n",
        "  for page_num in range(len(doc)):\n",
        "    page = doc.load_page(page_num)\n",
        "    text = page.get_text(\"text\")\n",
        "    full_text += text + \"\\n\"\n",
        "  return full_text\n",
        "\n",
        "full_text = extract_text_with_ocr(\"human-nutrition-text.pdf\")\n",
        "\n",
        "combined_doc = Document(text=full_text, metadata={\"source\":\"human-nutrition-text.pdf\"})"
      ],
      "metadata": {
        "id": "2Bvf31vQcqPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pT2VyVu3hh-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocessing and chunking"
      ],
      "metadata": {
        "id": "qV2evkgNTzkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###chunking the text"
      ],
      "metadata": {
        "id": "M2lrz16uWypI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,#target size of each chunk\n",
        "    chunk_overlap=0,#overlap between chunks for context continuity\n",
        "    separators = [\". \"]#split by sections, paragraphs, lines...\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(combined_doc.text)"
      ],
      "metadata": {
        "id": "P2LKHxwdUmbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###convert chunks into llamaIndex documents"
      ],
      "metadata": {
        "id": "2M0HLWIdt9QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_docs = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "  doc = Document(\n",
        "      text=chunk,\n",
        "      metadata={\n",
        "          \"source\":\"human-nutrition-text.pdf\",\n",
        "          \"chunk_id\":i,\n",
        "          \"char_count\":len(chunk),\n",
        "          \"token_count\":len(chunk)/4\n",
        "      }\n",
        "  )\n",
        "  chunked_docs.append(doc)\n",
        "\n",
        "print(len(chunked_docs))"
      ],
      "metadata": {
        "id": "wocTAgtzX6Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###cleaning each chunk"
      ],
      "metadata": {
        "id": "fu1Qd35QcZS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_chunk(text):\n",
        "  #remove page numbers\n",
        "  text = re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
        "  #replace newlines and extra spaces\n",
        "  text = re.sub(r\"\\n+\", \" \", text) #replace new lines with space\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip() #collapse multpile spaces\n",
        "  return text\n",
        "\n",
        "#clean each chunk\n",
        "cleaned_chunked_docs = []\n",
        "for doc in chunked_docs:\n",
        "  cleaned_text = clean_chunk(doc.text)\n",
        "  cleaned_doc = Document(\n",
        "      text = cleaned_text,\n",
        "      metadata = doc.metadata\n",
        "  )\n",
        "  cleaned_chunked_docs.append(cleaned_doc)\n"
      ],
      "metadata": {
        "id": "fq4bepnkw45p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generating the embeddings"
      ],
      "metadata": {
        "id": "wV5CJwLO71-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "\n",
        "embeddings = []\n",
        "for doc in cleaned_chunked_docs:\n",
        "  embedding = embedding_model.encode(doc.text)\n",
        "  embeddings.append(embedding)\n"
      ],
      "metadata": {
        "id": "W4fmFW-I8CKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing the embeddings in a vector database (FAISS)"
      ],
      "metadata": {
        "id": "jWxeg60A_f89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "#convert embeddings into numpy array\n",
        "embeddings_array = np.array(embeddings).astype('float32') #FAISS requires float32\n",
        "\n",
        "#create a FAISS index\n",
        "dimension = embeddings_array.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "index.add(embeddings_array)\n",
        "\n",
        "#save the index to a file\n",
        "faiss.write_index(index, \"faiss_index.index\")\n"
      ],
      "metadata": {
        "id": "OHb_I4w2Aws0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###building BM25 index\n",
        "keyword-based search algorithm"
      ],
      "metadata": {
        "id": "rK-OXkmgePH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from rank_bm25 import BM25Okapi\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# #preprocess chunks for bm25\n",
        "# tokenized_chunks = [word_tokenize(doc.text.lower()) for doc in cleaned_chunked_docs]\n",
        "# bm25 = BM25Okapi(tokenized_chunks)\n",
        "\n",
        "# def bm25_search(query, top_k=5):\n",
        "#   tokenized_query = word_tokenize(query.lower())\n",
        "#   scores = bm25.get_scores(tokenized_query)\n",
        "#   top_indices = scores.argsort()[-top_k:][::-1]\n",
        "#   return top_indices.tolist()"
      ],
      "metadata": {
        "id": "ihazjdf-eKnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###saving chunk texts and bm25 retriever to a file"
      ],
      "metadata": {
        "id": "7zgvgaequIbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# import json\n",
        "\n",
        "# chunk_texts = [doc.text for doc in cleaned_chunked_docs]\n",
        "# with open(\"chunk_texts.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(chunk_texts, f)\n",
        "\n",
        "# with open(\"bm25_retriever.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(bm25, f)"
      ],
      "metadata": {
        "id": "WkcEjAPIuEF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###creating FAISS retriever class"
      ],
      "metadata": {
        "id": "J1bmOxwCo4Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.schema.document import Document\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pydantic import Field\n",
        "from typing import List\n",
        "\n",
        "# Create FAISS retriever\n",
        "class FAISSRetriever(BaseRetriever):\n",
        "  index: faiss.Index = Field(index)\n",
        "  chunk_texts: List[str] = Field(cleaned_chunked_docs)\n",
        "\n",
        "  def __init__(self, index: faiss.Index, chunk_texts:list):\n",
        "        super().__init__()\n",
        "        self.index = index\n",
        "        self.chunk_texts = chunk_texts\n",
        "\n",
        "  def _get_relevant_documents(self, query:str, top_k:int=5) -> list[Document]:\n",
        "        query_embedding = embedding_model.encode(query).astype('float32').reshape(1, -1) #generate query embedding\n",
        "        distances, indices = self.index.search(query_embedding, top_k) #search FAISS index\n",
        "        return [ #return documents as list of document objects\n",
        "            Document(\n",
        "                page_content = self.chunk_texts[idx],\n",
        "                metadata = {\"source\":\"human-nutrition-text.pdf\"}\n",
        "            )\n",
        "            for idx in indices[0]\n",
        "        ]"
      ],
      "metadata": {
        "id": "bJbiG-3PoEET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid search implementation"
      ],
      "metadata": {
        "id": "6IVEw4HHhhHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# Create BM25 retriever\n",
        "bm25_retriever = BM25Retriever.from_texts(\n",
        "    texts=[doc.text for doc in cleaned_chunked_docs],\n",
        "    tokenizer=word_tokenize\n",
        ")\n",
        "bm25_retriever.k = 5  # Number of BM25 results\n",
        "\n",
        "#create faiss retriver (inherits from BaseRetriever)\n",
        "faiss_retriever = FAISSRetriever(index, [doc.text for doc in cleaned_chunked_docs])\n",
        "\n",
        "#combine retrievers\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        "    weights = [0.6,0.4]\n",
        ")\n",
        "\n",
        "#perform hybrid search\n",
        "query = \"role of fibers\"\n",
        "hybrid_results = ensemble_retriever.invoke(query,top_k=5)\n",
        "\n",
        "#extract texts from hybrid results\n",
        "hybrid_texts = [doc.page_content for doc in hybrid_results]"
      ],
      "metadata": {
        "id": "PA1n06TchfbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"bm25_retriever.pkl\", \"wb\") as f:\n",
        "  pickle.dump(bm25_retriever, f)"
      ],
      "metadata": {
        "id": "cILFXYRv_Q_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MSHGGt6j3fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integrating a Reranker"
      ],
      "metadata": {
        "id": "iCP6n_Ynxah-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "reranker_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "def rerank(query, documents, top_k=5):\n",
        "  #create query chunk pairs\n",
        "  pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "  #perdict relevance scores\n",
        "  scores = reranker_model.predict(pairs)\n",
        "\n",
        "  #sort documents by score\n",
        "  scored_docs = list(zip(documents, scores))\n",
        "  scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  #return top_k reranked documents\n",
        "  return [doc for doc, _ in scored_docs[:top_k]]"
      ],
      "metadata": {
        "id": "9RAslqa4-ZSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform hybrid search\n",
        "query = \"what are the benefits of antioxydants?\"\n",
        "hybrid_results = ensemble_retriever.invoke(query, top_k=10)\n",
        "\n",
        "#rerank the hybrid results\n",
        "reranked_results = rerank(query, hybrid_results, top_k=5)\n",
        "\n",
        "#extract text from reranked results\n",
        "reranked_texts = [doc.page_content for doc in reranked_results]\n",
        "\n",
        "print(\"reranked chunks\")\n",
        "for i, text in enumerate(reranked_texts):\n",
        "  print(f\"chunk {i+1}: {text}\")"
      ],
      "metadata": {
        "id": "x6Cz885I-vIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_context(texts):\n",
        "  context = \"\"\n",
        "  for i, text in enumerate(texts, start=1):\n",
        "    context += f\"chunk {i}:\\n {text}\\n\\n\"\n",
        "  return context.strip()"
      ],
      "metadata": {
        "id": "HaGcpcyaUVw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating response with an LLM"
      ],
      "metadata": {
        "id": "ojT8TkMtArdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "#Load the quantized LLM\n",
        "model_id = \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Lw9qknN-Bbnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=750,\n",
        "    temperature=0.7, #control creativity\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "#wrap in LangChain's HuggingFacePipeline\n",
        "llm  = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "MZRvLvkxVClE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###define the prompt template"
      ],
      "metadata": {
        "id": "IsqZcM9gFsQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template = \"\"\"\n",
        "    You are a nutrition expert. Answer the question based ***only*** on the provided context.\n",
        "    if the context does not contain the answer, respond with \"I dont't know\"\n",
        "    ***NOTE***:you can use your knowledge to generate some ***nutrition** related information IF AND ONLY IF the provided context did not contain much relevant info\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables=[\"context\",\"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "T-JzVmY6Rmti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example query\n",
        "query = \"\"\"explain the role of proteins in the body, and how can someone increase their protein intake, cite from which chapter or section you brought the info\n",
        "     \"\"\"\n",
        "# Retrieve, rerank and construct context\n",
        "hybrid_results = ensemble_retriever.invoke(query, top_k=10)\n",
        "reranked_results = rerank(query, hybrid_results,top_k=5)\n",
        "context = construct_context([doc.page_content for doc in reranked_results])\n",
        "\n",
        "#Format the prompt\n",
        "prompt = prompt_template.format(context=context, question=query)\n",
        "\n",
        "#generate the response\n",
        "response = llm(prompt)\n",
        "\n",
        "#clean and display the response\n",
        "cleaned_response = response.strip().replace(\"</s>\", \"\")\n",
        "print(f'Answer:\\n{cleaned_response}')"
      ],
      "metadata": {
        "id": "jWdqVGhUSneD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_pipeline.py\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import faiss\n",
        "import pickle\n",
        "from langchain.schema.document import Document\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from pydantic import Field\n",
        "from typing import List\n",
        "\n",
        "# Load components from disk\n",
        "index = faiss.read_index(\"faiss_index.index\")  # FAISS index\n",
        "with open(\"chunk_texts.pkl\", \"rb\") as f:\n",
        "    chunk_texts = pickle.load(f)  # List of cleaned text chunks\n",
        "\n",
        "# Load BM25 retriever (LangChain format)\n",
        "with open(\"bm25_retriever.pkl\", \"rb\") as f:\n",
        "    bm25_retriever = pickle.load(f)\n",
        "\n",
        "# Initialize models\n",
        "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")  # Load embedding model\n",
        "reranker_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# Initialize LLM\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model_id = \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=750)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create FAISS retriever\n",
        "class FAISSRetriever(BaseRetriever):\n",
        "  index: faiss.Index = Field(index)\n",
        "  chunk_texts: List[str] = Field(chunk_texts)\n",
        "\n",
        "  def __init__(self, index: faiss.Index, chunk_texts:list):\n",
        "        super().__init__()\n",
        "        self.index = index\n",
        "        self.chunk_texts = chunk_texts\n",
        "\n",
        "  def _get_relevant_documents(self, query:str, top_k:int=5) -> list[Document]:\n",
        "        query_embedding = embedding_model.encode(query).astype('float32').reshape(1, -1) #generate query embedding\n",
        "        distances, indices = self.index.search(query_embedding, top_k) #search FAISS index\n",
        "        return [ #return documents as list of document objects\n",
        "            Document(\n",
        "                page_content = self.chunk_texts[idx],\n",
        "                metadata = {\"source\":\"human-nutrition-text.pdf\"}\n",
        "            )\n",
        "            for idx in indices[0]\n",
        "        ]\n",
        "faiss_retriever = FAISSRetriever(index, chunk_texts)\n",
        "\n",
        "# Combine retrievers\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "# Hybrid search\n",
        "def hybrid_search(query, top_k=5):\n",
        "    return ensemble_retriever.invoke(query, top_k=top_k)\n",
        "\n",
        "# Rerank results\n",
        "def rerank(query, documents, top_k=5):\n",
        "    pairs = [[query, doc.page_content] for doc in documents]\n",
        "    scores = reranker_model.predict(pairs)\n",
        "    scored_docs = list(zip(documents, scores))\n",
        "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [doc for doc, _ in scored_docs[:top_k]]\n",
        "\n",
        "# Construct context\n",
        "def construct_context(texts):\n",
        "    context = \"\"\n",
        "    for i, text in enumerate(texts, start=1):\n",
        "        context += f\"Chunk {i}:\\n{text}\\n\\n\"\n",
        "    return context.strip()\n",
        "\n",
        "# Generate response\n",
        "def generate_response(query, context):\n",
        "    prompt_template = \"\"\"\n",
        "    You are a nutrition expert. Answer the question based **only** on the provided context.\n",
        "    If the context does not contain the answer, respond with \"I don't know.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    prompt = prompt_template.format(context=context, question=query)\n",
        "    return llm(prompt).strip()"
      ],
      "metadata": {
        "id": "o8G8Wu1riNHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from rag_pipeline import (\n",
        "    hybrid_search,\n",
        "    rerank,\n",
        "    construct_context,\n",
        "    generate_response\n",
        ")\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "  query:str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask(request: QueryRequest):\n",
        "  query = request.query\n",
        "  results = hybrid_search(query,top_k=10)\n",
        "  reranked_results = rerank(query,results,top_k=5)\n",
        "  context = construct_context([doc.page_content for doc in reranked_results])\n",
        "  answer = generate_response(query, context)\n",
        "  return {\"answer\":answer}"
      ],
      "metadata": {
        "id": "MT0sPEm8pv0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run rag_pipeline.py"
      ],
      "metadata": {
        "id": "ExJY5P4IItpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import functions\n",
        "from rag_pipeline import hybrid_search, rerank, construct_context, generate_response\n",
        "\n",
        "# Example query\n",
        "query = \"What are the benefits of antioxidants?\"\n",
        "\n",
        "# Step 1: Hybrid search\n",
        "results = hybrid_search(query, top_k=10)\n",
        "\n",
        "# Step 2: Rerank results\n",
        "reranked_results = rerank(query, results, top_k=5)\n",
        "\n",
        "# Step 3: Construct context\n",
        "context = construct_context([doc.page_content for doc in reranked_results])\n",
        "\n",
        "# Step 4: Generate response\n",
        "response = generate_response(query, context)\n",
        "\n",
        "# Print the final answer\n",
        "print(\"Query:\", query)\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "id": "WL40hdNsMeG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Start Uvicorn in the background\n",
        "uvicorn_process = subprocess.Popen(\n",
        "    [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        ")\n",
        "\n",
        "print(\"Uvicorn server started in the background.\")"
      ],
      "metadata": {
        "id": "219X0S8KQthn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok token\n",
        "ngrok.set_auth_token(\"2u0s2RgPMdfotGIpYtjj7ZjyCVf_njGnsKoRVivYDddyKqS\")\n",
        "\n",
        "print(\"ngrok authenticated successfully.\")"
      ],
      "metadata": {
        "id": "QEA-8zYlSDlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Connect ngrok to port 8000\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "zn0o04kJUnKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"https://255d-34-143-162-16.ngrok-free.app/ask\",  # Replace with your ngrok URL\n",
        "    json={\"query\": \"What are the benefits of antioxidants?\"}\n",
        ")\n",
        "\n",
        "# Print the raw response\n",
        "print(\"Status Code:\", response.status_code)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "ncrBsOiOVm6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn_process.terminate()\n",
        "print(\"Uvicorn server stopped.\")"
      ],
      "metadata": {
        "id": "fLNFJd0cVq42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xAOHDdWX9oA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}